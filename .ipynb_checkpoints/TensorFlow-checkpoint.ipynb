{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with TensorFlow\n",
    "\n",
    "In this notebook we will implement some more complicated neural networks. Rather than doing everything from scratch as we have up to now, we will make use of [TensorFlow 2](https://www.tensorflow.org/) and the [Keras](https://keras.io) high level interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing TensorFlow and Keras\n",
    "\n",
    "TensorFlow and Keras are not included with the base Anaconda install, but can be easily installed by running the following commands on the Anaconda Command Prompt/terminal window:\n",
    "```\n",
    "conda install notebook jupyterlab nb_conda_kernels\n",
    "conda create -n tf tensorflow ipykernel mkl\n",
    "```\n",
    "Once this has been done, you should be able to select the `Python [conda env:tf]` kernel from the Kernel->Change Kernel menu item at the top of this notebook. Then, we import TensorFlow package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple network with TensorFlow\n",
    "\n",
    "We will start by creating a very simple fully connected feedforward network using TensorFlow/Keras. The network will mimic the one we implemented previously, but TensorFlow/Keras will take care of most of the details for us.\n",
    "\n",
    "First, let us load the MNIST digits dataset that we will be using to train our network. This is available directly within Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes as a set of integers in the range [0,255] representing the shade of gray of a given pixel. Let's first rescale them to be in the range [0,1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to convert the y values from integers to a one-hot representation as a 10-component vector with one non-zero entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vec = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_vec = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a neural network model using Keras. This uses a very simple high-level modular structure where we only have the specify the layers in our model and the properties of each layer. The layers we will have are as follows:\n",
    "1. Input layer: This will be a 28x28 matrix of numbers.\n",
    "2. `Flatten` layer: Convert our 28x28 pixel image into an array of size 784.\n",
    "3. `Dense` layer: a fully-connected layer of the type we have been using up to now. We will use 30 neurons and the sigmoid activation function.\n",
    "4. `Dense` layer: fully-connected output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antho\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(30, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile this model, specifying the optimization algorithm (ADAM) and loss function (mean squared error) to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=\"mean_squared_error\",\n",
    "              metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model with our training data. We will run for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.7691 - loss: 0.0390\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9263 - loss: 0.0120\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9404 - loss: 0.0096\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9497 - loss: 0.0082\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9553 - loss: 0.0073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cbd0fe8c40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train_vec, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check the accuracy of our model against the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007564563769847155, 0.9514999985694885]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test_vec, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has 95.5% accuracy, consistent with what was found during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "Experiment with this network:\n",
    "1. Change the number of neurons in the hidden layer.\n",
    "2. Add more hidden layers.\n",
    "3. Change the activation function in the hidden layer to `relu` (for examples see the list of [Keras Layer Activation Functions](https://keras.io/api/layers/activations/)).\n",
    "4. Change the activation in the output layer to something other than `softmax`.\n",
    "5. Change the loss function (for examples see the list of [Keras Loss Functions](https://keras.io/api/losses/)).\n",
    "How does the performance of your network change with these modifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - categorical_accuracy: 0.6417 - loss: 0.0586\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.8863 - loss: 0.0193\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9050 - loss: 0.0153\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9133 - loss: 0.0139\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9220 - loss: 0.0125\n",
      "\n",
      "Model with 10 hidden neurons:\n",
      "Test Loss: 0.0122\n",
      "Test Accuracy: 0.9229\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.7626 - loss: 0.0391\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9244 - loss: 0.0122\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9401 - loss: 0.0097\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9486 - loss: 0.0084\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9559 - loss: 0.0073\n",
      "\n",
      "Model with 30 hidden neurons:\n",
      "Test Loss: 0.0073\n",
      "Test Accuracy: 0.9517\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - categorical_accuracy: 0.7942 - loss: 0.0336\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9350 - loss: 0.0105\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9487 - loss: 0.0084\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9569 - loss: 0.0071\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9629 - loss: 0.0061\n",
      "\n",
      "Model with 50 hidden neurons:\n",
      "Test Loss: 0.0063\n",
      "Test Accuracy: 0.9597\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - categorical_accuracy: 0.8222 - loss: 0.0284\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9419 - loss: 0.0093\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9552 - loss: 0.0072\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9664 - loss: 0.0056\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9710 - loss: 0.0049\n",
      "\n",
      "Model with 100 hidden neurons:\n",
      "Test Loss: 0.0052\n",
      "Test Accuracy: 0.9672\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Rescale data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# One-hot encode target labels\n",
    "y_train_vec = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_vec = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Function to create and train a model\n",
    "def create_and_train_model(hidden_neurons=30, epochs=5):\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(hidden_neurons, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=[\"categorical_accuracy\"])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train_vec, epochs=epochs, verbose=True)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    evaluation = model.evaluate(x_test, y_test_vec, verbose=False)\n",
    "    \n",
    "    print(f\"\\nModel with {hidden_neurons} hidden neurons:\")\n",
    "    print(f\"Test Loss: {evaluation[0]:.4f}\")\n",
    "    print(f\"Test Accuracy: {evaluation[1]:.4f}\")\n",
    "    return history, evaluation\n",
    "\n",
    "# Experiment with different numbers of hidden neurons\n",
    "hidden_neurons_list = [10, 30, 50, 100]\n",
    "for neurons in hidden_neurons_list:\n",
    "    create_and_train_model(hidden_neurons=neurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - categorical_accuracy: 0.5171 - loss: 0.0614\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9158 - loss: 0.0136\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9405 - loss: 0.0094\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9503 - loss: 0.0078\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9566 - loss: 0.0068\n",
      "Test Loss: 0.0072, Test Accuracy: 0.9532\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the input data to the range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_vec = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_vec = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define a model with more hidden layers, using sigmoid activation\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(30, activation='sigmoid'),  # First hidden layer\n",
    "    tf.keras.layers.Dense(30, activation='sigmoid'),  # Second hidden layer\n",
    "    tf.keras.layers.Dense(30, activation='sigmoid'),  # Third hidden layer\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=\"mean_squared_error\",\n",
    "              metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train_vec, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test_vec, verbose=False)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - categorical_accuracy: 0.7988 - loss: 0.0284\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9449 - loss: 0.0087\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9569 - loss: 0.0067\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9617 - loss: 0.0059\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9671 - loss: 0.0052\n",
      "Test Loss: 0.0065, Test Accuracy: 0.9588\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the input data to the range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_vec = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_vec = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define a model with hidden layers using ReLU activation\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(30, activation='relu'),  # First hidden layer with ReLU\n",
    "    tf.keras.layers.Dense(30, activation='relu'),  # Second hidden layer with ReLU\n",
    "    tf.keras.layers.Dense(30, activation='relu'),  # Third hidden layer with ReLU\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=\"mean_squared_error\",\n",
    "              metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train_vec, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test_vec, verbose=False)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.7197 - loss: 0.1643\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - categorical_accuracy: 0.9440 - loss: 0.0364\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9579 - loss: 0.0274\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - categorical_accuracy: 0.9646 - loss: 0.0224\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9703 - loss: 0.0196\n",
      "Test Loss: 0.0217, Test Accuracy: 0.9678\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the input data to the range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_vec = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_vec = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define a model with sigmoid activation in the output layer\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(30, activation='relu'),  # First hidden layer with ReLU\n",
    "    tf.keras.layers.Dense(30, activation='relu'),  # Second hidden layer with ReLU\n",
    "    tf.keras.layers.Dense(30, activation='relu'),  # Third hidden layer with ReLU\n",
    "    tf.keras.layers.Dense(10, activation='sigmoid')  # Output layer with Sigmoid\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=\"binary_crossentropy\",  # Binary crossentropy for sigmoid output\n",
    "              metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train_vec, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test_vec, verbose=False)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used sigmoid instead, which is commonly used for binary classification tasks but can also be adapted for multi-class classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with loss function: categorical_crossentropy\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - categorical_accuracy: 0.8077 - loss: 0.6336\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9483 - loss: 0.1763\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9596 - loss: 0.1328\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - categorical_accuracy: 0.9661 - loss: 0.1127\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - categorical_accuracy: 0.9698 - loss: 0.0986\n",
      "Test Loss: 0.1247, Test Accuracy: 0.9643\n",
      "\n",
      "Training with loss function: mean_squared_error\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - categorical_accuracy: 0.7986 - loss: 0.0277\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - categorical_accuracy: 0.9468 - loss: 0.0082\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9571 - loss: 0.0068\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - categorical_accuracy: 0.9617 - loss: 0.0060\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - categorical_accuracy: 0.9668 - loss: 0.0053\n",
      "Test Loss: 0.0068, Test Accuracy: 0.9559\n",
      "\n",
      "Training with loss function: mean_absolute_error\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - categorical_accuracy: 0.7335 - loss: 0.0609\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - categorical_accuracy: 0.9281 - loss: 0.0155\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - categorical_accuracy: 0.9401 - loss: 0.0127\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9506 - loss: 0.0103\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9534 - loss: 0.0097\n",
      "Test Loss: 0.0112, Test Accuracy: 0.9456\n",
      "\n",
      "Training with loss function: hinge\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - categorical_accuracy: 0.6477 - loss: 0.9763\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.7563 - loss: 0.9494\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.7890 - loss: 0.9426\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.8691 - loss: 0.9266\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - categorical_accuracy: 0.9456 - loss: 0.9113\n",
      "Test Loss: 0.9108, Test Accuracy: 0.9472\n",
      "\n",
      "Summary of Results:\n",
      "categorical_crossentropy: Test Loss = 0.1247, Test Accuracy = 0.9643\n",
      "mean_squared_error: Test Loss = 0.0068, Test Accuracy = 0.9559\n",
      "mean_absolute_error: Test Loss = 0.0112, Test Accuracy = 0.9456\n",
      "hinge: Test Loss = 0.9108, Test Accuracy = 0.9472\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the input data to the range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_vec = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_vec = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define a function to create and train a model with a specific loss function\n",
    "def train_model_with_loss(loss_function):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(30, activation='relu'),  # First hidden layer with ReLU\n",
    "        tf.keras.layers.Dense(30, activation='relu'),  # Second hidden layer with ReLU\n",
    "        tf.keras.layers.Dense(30, activation='relu'),  # Third hidden layer with ReLU\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Output layer with Softmax\n",
    "    ])\n",
    "    \n",
    "    # Compile the model with the specified loss function\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_function,  # Use the loss function passed to the function\n",
    "                  metrics=[\"categorical_accuracy\"])\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"\\nTraining with loss function: {loss_function}\")\n",
    "    model.fit(x_train, y_train_vec, epochs=5, verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test_vec, verbose=False)\n",
    "    print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "    return loss, accuracy\n",
    "\n",
    "# Experiment with different loss functions\n",
    "loss_functions = [\n",
    "    \"categorical_crossentropy\",  # Standard for multi-class classification\n",
    "    \"mean_squared_error\",        # Used previously, though not optimal for classification\n",
    "    \"mean_absolute_error\",       # Less common but measures average absolute differences\n",
    "    \"hinge\"                      # Suitable for binary classification or margin-based tasks\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for loss_fn in loss_functions:\n",
    "    results[loss_fn] = train_model_with_loss(loss_fn)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nSummary of Results:\")\n",
    "for loss_fn, (loss, accuracy) in results.items():\n",
    "    print(f\"{loss_fn}: Test Loss = {loss:.4f}, Test Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Loss Functions:\n",
    "\n",
    "1. **categorical_crossentropy**:\n",
    "   - Most commonly used for multi-class classification with one-hot encoded labels.\n",
    "   - Measures the log loss between predicted and true probability distributions.\n",
    "\n",
    "2. **mean_squared_error**:\n",
    "   - Measures the squared differences between predicted and true values.\n",
    "   - Better suited for regression tasks but included here for comparison.\n",
    "\n",
    "3. **mean_absolute_error**:\n",
    "   - Measures the average absolute differences between predicted and true values.\n",
    "   - Similar to mean squared error but penalizes outliers less harshly.\n",
    "\n",
    "4. **hinge**:\n",
    "   - Typically used for binary classification tasks.\n",
    "   - Evaluates the margin between predicted and true class labels, with penalties for violations.\n",
    "\n",
    "### Output:\n",
    "At the end of the training, the results will display the **test loss** and **test accuracy** for each loss function. This allows you to compare the effectiveness of different loss functions for your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Performance with Different Loss Functions\n",
    "\n",
    "## Code Summary and Observations for Loss Functions:\n",
    "\n",
    "### 1. **categorical_crossentropy**\n",
    "- **Usage**: Best suited for multi-class classification tasks with one-hot encoded labels.\n",
    "- **Performance**:\n",
    "  - Test Loss: 0.1247\n",
    "  - Test Accuracy: 96.43%\n",
    "- **Observation**: This is the most effective loss function for the MNIST classification problem, balancing precision and stability.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **mean_squared_error**\n",
    "- **Usage**: Measures squared differences between predicted and true values; often used for regression.\n",
    "- **Performance**:\n",
    "  - Test Loss: 0.0068\n",
    "  - Test Accuracy: 95.59%\n",
    "- **Observation**: While it works for classification, it's less effective than categorical crossentropy due to differences in optimization dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **mean_absolute_error**\n",
    "- **Usage**: Measures average absolute differences; penalizes outliers less harshly compared to mean squared error.\n",
    "- **Performance**:\n",
    "  - Test Loss: 0.0112\n",
    "  - Test Accuracy: 94.56%\n",
    "- **Observation**: Shows slightly lower accuracy and stability compared to mean squared error.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **hinge**\n",
    "- **Usage**: Evaluates margins between predicted and true labels; commonly used in binary classification.\n",
    "- **Performance**:\n",
    "  - Test Loss: 0.9108\n",
    "  - Test Accuracy: 94.72%\n",
    "- **Observation**: Performs surprisingly well despite being tailored for binary classification; however, loss remains high due to incompatibility with multi-class tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Summary:\n",
    "\n",
    "| Loss Function           | Test Loss | Test Accuracy |\n",
    "|-------------------------|-----------|---------------|\n",
    "| categorical_crossentropy | 0.1247    | **96.43%**    |\n",
    "| mean_squared_error       | 0.0068    | 95.59%        |\n",
    "| mean_absolute_error      | 0.0112    | 94.56%        |\n",
    "| hinge                    | 0.9108    | 94.72%        |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights:\n",
    "- **categorical_crossentropy** consistently outperforms other loss functions for the MNIST dataset due to its alignment with the task's probabilistic nature.\n",
    "- Regression-based loss functions like **mean_squared_error** and **mean_absolute_error** are viable alternatives but less optimal.\n",
    "- **hinge loss**, while effective for binary classification, is less suited for multi-class problems like MNIST.\n",
    "\n",
    "These observations highlight the importance of selecting the correct loss function based on the problem domain. Let me know if you would like further experiments or details!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
