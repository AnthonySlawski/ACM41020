{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In this notebook we will implement a convolutional neural network. Rather than doing everything from scratch we will make use of [TensorFlow 2](https://www.tensorflow.org/) and the [Keras](https://keras.io) high level interface.\n",
    "\n",
    "## Convolution neural network for MNIST dataset\n",
    "\n",
    "Implement the neural network in \"[Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\", by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. The [Keras Layer documentation](https://keras.io/api/layers/) includes information about the layers supported. In particular, [`Conv2D`](https://keras.io/api/layers/convolution_layers/convolution2d) and [`MaxPooling2D`](https://keras.io/api/layers/pooling_layers/max_pooling2d) layers may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "First, let us load the MNIST digits dataset that we will be using to train our network. This is available directly within Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes as a set of integers in the range [0,255] representing the shade of gray of a given pixel. Let's first rescale them to be in the range [0,1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to reshape the input data to make the images 28 x 28 x 1 rather than 28 x 28. This is beacause more generally we might have 28 x 28 x 3 to account for the three colour channels (red, green, blue) in an image, but here we have only one grayscale channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train[..., np.newaxis]\n",
    "X_test = x_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct our network with three convolution layers, two pooling layers and fully-connected layers at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the CNN model\n",
    "model = tf.keras.models.Sequential([\n",
    "    # First convolutional layer\n",
    "    tf.keras.layers.Conv2D(6,(5,5),input_shape=(28,28,1),padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Second convolutional layer\n",
    "    tf.keras.layers.Conv2D(16,(5,5), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Third \n",
    "    tf.keras.layers.Conv2D(120,(5,5), activation='relu'),\n",
    "    # Flattening layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Fully connected layers\n",
    "    tf.keras.layers.Dense(84, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer with 10 classes\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## 1. What is a Filter in CNNs?\n",
    "A **filter** (or **kernel**) is a small matrix of numbers (parameters) that slides over the input image and performs a convolution operation. The result is a **feature map**, which highlights specific patterns or features in the image, such as edges, corners, or textures.\n",
    "\n",
    "### Key Characteristics of a Filter:\n",
    "- **Size:**\n",
    "  - Filters are typically small, like $3 \\times 3$, $5 \\times 5$, or $7 \\times 7$, relative to the input size.\n",
    "  - For a $28 \\times 28$ MNIST image, a $5 \\times 5$ filter scans over $5 \\times 5$ sections of the image.\n",
    "\n",
    "- **Convolution Operation:**\n",
    "  - At each position, the filter performs an **element-wise multiplication** with the input pixels it overlaps and sums the results.\n",
    "  - The filter then moves (or **slides**) to the next position by a step size called the **stride**.\n",
    "\n",
    "### Example:\n",
    "Let’s say the input image is:\n",
    "$$\n",
    "\\text{Input Image: }\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 0 & 1 \\\\\n",
    "3 & 4 & 1 & 0 \\\\\n",
    "0 & 1 & 3 & 2 \\\\\n",
    "1 & 0 & 2 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the filter (kernel) is:\n",
    "$$\n",
    "\\text{Filter: }\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "-1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To compute the convolution:\n",
    "1. Place the filter over the top-left corner of the input.\n",
    "2. Perform element-wise multiplication:\n",
    "   $$\n",
    "   (1 \\cdot 1) + (2 \\cdot 0) + (3 \\cdot -1) + (4 \\cdot 1) = 1 + 0 - 3 + 4 = 2\n",
    "   $$\n",
    "3. Slide the filter to the right by one step (stride of 1) and repeat.\n",
    "\n",
    "The result is a **feature map**, smaller than the original image.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Padding in CNNs\n",
    "**Padding** determines how the edges of the input are handled during convolution. Without padding, the convolution operation reduces the size of the output feature map.\n",
    "\n",
    "### Types of Padding:\n",
    "1. **Same Padding:**\n",
    "   - Adds zeros around the edges of the input to ensure the output size is the same as the input size.\n",
    "   - If the input is $28 \\times 28$ and the filter is $3 \\times 3$, the padding ensures the output remains $28 \\times 28$.\n",
    "\n",
    "2. **Valid Padding:**\n",
    "   - No padding is added.\n",
    "   - The output size is reduced because the filter cannot extend beyond the input edges.\n",
    "   - For example, a $28 \\times 28$ input with a $3 \\times 3$ filter results in an output size of $26 \\times 26$ (subtracting $2$ rows and columns).\n",
    "\n",
    "### Why Use Padding?\n",
    "- **Same Padding:**\n",
    "  - Ensures features near the edges of the image are treated equally.\n",
    "  - Preserves the spatial size of the output.\n",
    "\n",
    "- **Valid Padding:**\n",
    "  - Reduces computation by shrinking the feature map.\n",
    "  - Useful when the exact output size isn’t critical.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Pooling in CNNs\n",
    "Pooling is a **down-sampling** operation that reduces the spatial size of the feature maps. This simplifies computation and helps prevent overfitting.\n",
    "\n",
    "### Types of Pooling:\n",
    "1. **Max Pooling:**\n",
    "   - Divides the input into non-overlapping regions and takes the maximum value from each region.\n",
    "   - For example, for a $2 \\times 2$ region:\n",
    "     $$\n",
    "     \\text{Region: }\n",
    "     \\begin{bmatrix}\n",
    "     1 & 3 \\\\\n",
    "     2 & 4\n",
    "     \\end{bmatrix}\n",
    "     \\quad \\text{Max Value: } 4\n",
    "     $$\n",
    "\n",
    "2. **Average Pooling:**\n",
    "   - Takes the average of all values in the region.\n",
    "     $$\n",
    "     \\text{Average Value: } \\frac{1 + 3 + 2 + 4}{4} = 2.5\n",
    "     $$\n",
    "\n",
    "### Key Parameters:\n",
    "- **Pool Size:**\n",
    "  - Defines the size of the pooling region, e.g., $2 \\times 2$.\n",
    "- **Stride:**\n",
    "  - Determines how far the pooling window moves. Default is equal to the pool size, ensuring non-overlapping regions.\n",
    "\n",
    "### Why Use Pooling?\n",
    "- Reduces the spatial size of feature maps.\n",
    "- Focuses on dominant features while discarding unnecessary details.\n",
    "- Makes the model robust to small translations and distortions.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Fully Connected (Dense) Layers\n",
    "After convolution and pooling, the feature maps are flattened into a 1D vector and passed through fully connected (dense) layers.\n",
    "\n",
    "### Role of Dense Layers:\n",
    "- Combine all the extracted features to make predictions.\n",
    "- Each neuron in a dense layer connects to every value in the input vector, learning a weighted sum.\n",
    "\n",
    "### Why 84 Neurons?\n",
    "- The number $84$ is inspired by the **LeNet-5 architecture**, which used this size for its hidden layer.\n",
    "- It’s large enough to capture complex patterns while being computationally efficient.\n",
    "\n",
    "### Why 10 Neurons in the Output Layer?\n",
    "- The MNIST dataset has 10 classes (digits 0-9).\n",
    "- Each neuron corresponds to a class, and the output values represent the probabilities for each class.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Activation Functions\n",
    "Activation functions introduce **non-linearity** into the model, allowing it to learn complex patterns.\n",
    "\n",
    "### Common Activation Functions:\n",
    "- **ReLU (Rectified Linear Unit):**\n",
    "  $$\n",
    "  \\text{ReLU}(x) = \\max(0, x)\n",
    "  $$\n",
    "  - Helps prevent the vanishing gradient problem.\n",
    "- **Sigmoid:**\n",
    "  - Maps values to the range $(0, 1)$, often used in output layers for binary classification.\n",
    "- **Softmax:**\n",
    "  - Converts raw scores into probabilities for multi-class classification.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Why Does the Model Use This Architecture?\n",
    "1. **Convolutional Layers:**\n",
    "   - Detect hierarchical features (edges, shapes, patterns).\n",
    "   - Increasing filters (6, 16, 120) allows the network to learn more complex features.\n",
    "\n",
    "2. **Pooling Layers:**\n",
    "   - Reduce the spatial size of feature maps, focusing on the most important features.\n",
    "\n",
    "3. **Dense Layers:**\n",
    "   - Combine features to make the final prediction.\n",
    "\n",
    "4. **Output Layer:**\n",
    "   - Maps the features to probabilities for each digit class.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary:\n",
    "- **Filters:** Learn to detect features like edges and shapes.\n",
    "- **Kernel Size:** Defines the receptive field of each filter.\n",
    "- **Padding:** Determines how edges are handled.\n",
    "- **Pooling:** Reduces spatial size while preserving important features.\n",
    "- **Dense Layers:** Combine features for classification.\n",
    "- **Activation Functions:** Add non-linearity to learn complex patterns.\n",
    "\n",
    "This combination allows CNNs to process images efficiently and make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compile the model, specfiying sparse categorical cross-entropy loss and ADAM optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.8760 - loss: 0.3991\n",
      "Epoch 2/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9810 - loss: 0.0607\n",
      "Epoch 3/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9858 - loss: 0.0428\n",
      "Epoch 4/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9898 - loss: 0.0334\n",
      "Epoch 5/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9915 - loss: 0.0259\n",
      "Epoch 6/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9925 - loss: 0.0223\n",
      "Epoch 7/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9939 - loss: 0.0196\n",
      "Epoch 8/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9954 - loss: 0.0138\n",
      "Epoch 9/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9957 - loss: 0.0136\n",
      "Epoch 10/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9968 - loss: 0.0098\n",
      "Epoch 11/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9958 - loss: 0.0109\n",
      "Epoch 12/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9976 - loss: 0.0077\n",
      "Epoch 13/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9970 - loss: 0.0089\n",
      "Epoch 14/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9972 - loss: 0.0076\n",
      "Epoch 15/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9981 - loss: 0.0057\n",
      "Epoch 16/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0074\n",
      "Epoch 17/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9984 - loss: 0.0056\n",
      "Epoch 18/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.9975 - loss: 0.0078\n",
      "Epoch 19/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.9985 - loss: 0.0049\n",
      "Epoch 20/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9980 - loss: 0.0070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x12f82a35bb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for 20 epochs\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved 99.6% accuracy after training for 20 epochs. Let's check this against the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.052501361817121506, 0.9879999756813049]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 99%, so we may have slightly overtrained, but still have a highly accurate model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
